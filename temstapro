#!/usr/bin/env python3

# Program that makes thermostability predictions

from optparse import OptionParser
import sys
from datetime import datetime
import os
import prottrans_models
import data_process
import numpy
from torch.utils.data import DataLoader
from torch.utils.data import TensorDataset
from MLP import MLP_C2H2
import torch
import model_flow
import results

PT_MODEL_PATH = "Rostlab/prot_t5_xl_half_uniref50-enc"
DATASET = "major"
EMB_TYPE = "mean"
CLASSIFIER_TYPE = "imbal"
THRESHOLDS = ["40", "45", "50", "55", "60", "65"]
SEEDS = ["41", "42", "43", "44", "45"]
INPUT_SIZE = 1024
HIDDEN_LAYER_SIZES = [512, 256]
DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

TEMPERATURE_RANGES = ["<40", "[40-45)", "[45-50)", "[50-55)", "[55-60)", 
    "[60-65)", "65<="]

parser = OptionParser()

parser.add_option("--input-fasta", "-f", dest="fasta",
    default=None, help="path to the input FASTA file.")

parser.add_option("--embeddings-dir", "-e", dest="emb_dir",
    default=None, help="path to the directory to which embeddings "+\
    "files will be saved (cache).")

parser.add_option("--PT-directory", "-d", dest="pt_dir",
    default=None, help="path to the directory of ProtTrans model.")

parser.add_option("--temstapro-directory", "-t", dest="tsp_dir",
    default='./', help="path to the directory of TemStaPro program "+\
    "with its dependencies.")

parser.add_option("--mean-output", dest="mean_out",
    default=None, help="path to the output TSV file with mean predictions. "+\
    "Predictions made from the mean embeddings are always printed to STDOUT."+\
    " If this option is given, the output is directed to the given file")

parser.add_option("--per-res-output", dest="per_res_out",
    default=None, help="path to the output TSV file with per-residue "+\
    "predictions.")

parser.add_option("--per-segment-output", dest="per_segment_out",
    default=None, help="path to the output TSV file with per-residue "+\
    "predictions made for each segment of the sequence.")

parser.add_option("--segment-size", dest="segment_size",
    default=41, help="option to set the window size for average smoothening "+\
    "of per residue embeddings ('per-segment-output' option). Default: 41.")

parser.add_option("--window-size-predictions", "-w", 
    dest="window_size_predictions",
    default=81, help="option to set the window size for average smoothening "+\
    "of per residue predictions for plotting (option for 'per-res-output' "+\
    "and 'per-res-smooth-output'). Default: 81.")

parser.add_option("--per-residue-plot-dir", "-p", dest="plot_dir",
    default=None, help="path to the directory to which inferences "+\
    "plots will be saved (option for 'per-res-output' and "+\
    "'per-res-smooth-output' modes. Default: './'.")

parser.add_option("--curve-smoothening", "-c", dest="curve_smoothening",
    default=False, action="store_true", 
    help="option for 'per-res-smooth-output' run mode, which adjusts the "+\
    "plot by making an additional smoothening of the curve.")

parser.add_option("--portion-size", dest="portion_size",
    default=100, 
    help="option to set the portions', into which to divide the input "+\
    "of sequences, maximum size. If no division is needed, set the "+\
    "option to 0. Default: 100.")

(options, args) = parser.parse_args()

options.window_size_predictions = int(options.window_size_predictions)
options.segment_size = int(options.segment_size)

CLASSIFIERS_DIR = f"{options.tsp_dir}/models"

try:
    assert (options.fasta != None), f"{sys.argv[0]}: a FASTA file is required."
except AssertionError as message:
    print(message, file=sys.stderr)
    exit()

try:
    assert (options.pt_dir != None), f"{sys.argv[0]}: a path to the ProtTrans model location is required."
except AssertionError as message:
    print(message, file=sys.stderr)
    exit()

# Standardization of the FASTA file
(sequences, orig_headers, orig_seqs) = prottrans_models.process_FASTA(options.fasta)

# Loading the ProtTrans model
print("%s: beginning to load the model " % datetime.now(), file=sys.stderr)

if(os.path.isfile(f"{options.pt_dir}/pytorch_model.bin")):
    model = prottrans_models.get_pretrained_model(options.pt_dir)
else:
    if(not os.path.exists(f"{options.pt_dir}/")):
        os.system(f"mkdir -p {options.pt_dir}/")
    model = prottrans_models.get_pretrained_model(PT_MODEL_PATH)
    prottrans_models.save_pretrained_model(model, options.pt_dir)

tokenizer = prottrans_models.get_tokenizer(PT_MODEL_PATH)
print("%s: finished loading the model" % datetime.now(), file=sys.stderr)

# Dividing sequences into portions
options.portion_size = int(options.portion_size)
if(options.portion_size == 0): options.portion_size = len(sequences)

per_res_mode = (options.per_res_out or options.per_segment_out)

for i in range(0, len(list(sequences.keys())), options.portion_size):
    portion_keys = list(sequences.keys())[i:i+options.portion_size]
    
    sequences_portion = {}
    for key in portion_keys:
        sequences_portion[key] = sequences[key]

    # Check which sequences do not have embeddings generated
    if(options.emb_dir and os.path.exists(options.emb_dir)):
        seqs_wo_emb_portion = data_process.get_sequences_without_embeddings(
            sequences_portion, options.emb_dir, per_res=per_res_mode)
    else:
        seqs_wo_emb_portion = sequences_portion
   
    embeddings = {}
    per_res_dataset = {}
    per_res_sequences = {}
 
    if(len(seqs_wo_emb_portion)):
        gen_emb_start = datetime.now()
        print(f"{datetime.now()}: beginning to generate embeddings", file=sys.stderr)

        # Generating embeddings
        embeddings = prottrans_models.get_embeddings(model, tokenizer, 
            seqs_wo_emb_portion, 
            per_residue=per_res_mode, 
            per_protein=True)

        gen_emb_end = datetime.now()
        
        # If cache given, save embeddings
        if(options.emb_dir and os.path.exists(options.emb_dir)):
            if(per_res_mode):
                prottrans_models.save_embeddings(seqs_wo_emb_portion, embeddings,
                    options.emb_dir, "per_res")
            prottrans_models.save_embeddings(seqs_wo_emb_portion, embeddings, 
                options.emb_dir, "mean")
        elif(options.emb_dir and not os.path.exists(options.emb_dir)):
            print("The given directory (option -e) does not exist. Embeddings PT files "+\
                "will not be saved.", file=sys.stderr)

        try:
            # Printing embeddings generation statistics
            print(f"Portion {int(i/options.portion_size)+1}.", file=sys.stderr)
            print(f"{len(embeddings['mean_representations'].keys())}/{len(list(seqs_wo_emb_portion.keys()))}: "+\
                "sequences with generated mean embeddings",
                file=sys.stderr)
            print(f"{len(embeddings['per_res_representations'].keys())}/{len(list(seqs_wo_emb_portion.keys()))}: "+\
                "sequences with generated per-residue embeddings",
                file=sys.stderr)
            print("%s: time to generate embeddings" % (gen_emb_end - gen_emb_start),
                file=sys.stderr)
            print("%s: time to generate embeddings per protein" % ((gen_emb_end - \
                gen_emb_start)/len(embeddings["mean_representations"])),
                file=sys.stderr)
        except ZeroDivisionError:
            print(f"{sys.argv[0]}: no embeddings were generated.", file=sys.stderr)
            sys.exit(1)

    # Collecting the required type of embeddings
    dataset = data_process.collect_mean_embeddings(sequences_portion, 
        embeddings=embeddings, emb_dir=options.emb_dir, 
        input_size=INPUT_SIZE)

    if(options.per_res_out):
        per_res_dataset = data_process.collect_per_res_embeddings(sequences_portion, 
            orig_seqs, embeddings=embeddings, emb_dir=options.emb_dir, 
            input_size=INPUT_SIZE)
        per_res_sequences = per_res_dataset["z_test"]
    elif(options.per_segment_out):
        per_res_dataset = data_process.collect_per_res_embeddings(sequences_portion, 
            orig_seqs, embeddings=embeddings,
            emb_dir=options.emb_dir, input_size=INPUT_SIZE, smoothen=True, 
            window_size=options.segment_size)
        per_res_sequences = per_res_dataset["z_test"]

    test_loader, per_res_test_loader = model_flow.prepare_data_loaders([
        dataset, per_res_dataset], 'test')

    print("%s: beginning to make inferences" % datetime.now(), 
        file=sys.stderr)

    averaged_inferences, binary_inferences, labels, clashes = model_flow.prepare_inference_dictionaries(
        [sequences_portion, per_res_sequences])

    # Making inferences
    for j, loader in enumerate([test_loader, per_res_test_loader]):
        if(loader is None): break
        for threshold in THRESHOLDS:
            threshold_inferences = {}
            for seed in SEEDS:
                classifier = MLP_C2H2(INPUT_SIZE, HIDDEN_LAYER_SIZES[0], 
                    HIDDEN_LAYER_SIZES[1])
                model_path = "%s/%s_%s_%s-%s_s%s.pt" % (
                    CLASSIFIERS_DIR, EMB_TYPE, DATASET, 
                    CLASSIFIER_TYPE, threshold, seed)
                classifier.load_state_dict(torch.load(model_path))
                classifier.eval()

                classifier.to(DEVICE)

                threshold_inferences[seed] = model_flow.inference_epoch(classifier, 
                    loader, 
                    identifiers=list(averaged_inferences[j].keys()), device=DEVICE)
            # Taking average of the predictions 
            for seq in threshold_inferences["41"].keys():
                mean_prediction = 0
                for seed in SEEDS:
                     mean_prediction += threshold_inferences[seed][seq]
                mean_prediction /= len(SEEDS)
                averaged_inferences[j][seq].append(mean_prediction)
                binary_inferences[j][seq].append(round(mean_prediction))

    print("%s: finished making inferences" % datetime.now(), file=sys.stderr)
            
    # Processing results
    for j, loader in enumerate([test_loader, per_res_test_loader]):
        if(loader is None): break
        for seq in averaged_inferences[j].keys():
            labels[j][seq].append(results.get_temperature_label(
                averaged_inferences[j][seq], 
                TEMPERATURE_RANGES, left_hand=True))
            labels[j][seq].append(results.get_temperature_label(
                averaged_inferences[j][seq],
                TEMPERATURE_RANGES, left_hand=False))
            clashes[j][seq].append(results.detect_clash(averaged_inferences[j][seq],
                left_hand=True))

    # Processing printing of mean predictions
    if(options.mean_out):
        f_mean = open(options.mean_out, "w") if i == 0 else open(options.mean_out, "a")
    else:
        f_mean = sys.stdout

    if(i == 0): results.print_inferences_header(f_mean, options.fasta)

    results.print_inferences(averaged_inferences[0], binary_inferences[0], 
        orig_headers, labels[0], clashes[0], f_mean, orig_seqs, 
        options.fasta, "mean")

    # Printing per-residue inferences
    if(options.per_res_out):
        f_per_res = open(options.per_res_out, "w") if i == 0 else open(options.per_res_out, "a")
        if(i == 0): results.print_inferences_header(f_per_res, options.fasta)

        results.print_inferences(averaged_inferences[1], binary_inferences[1], 
            orig_headers, labels[1],
            clashes[1], f_per_res, per_res_sequences, options.fasta, "per-res")
    elif(options.per_segment_out):
        f_per_res = open(options.per_segment_out, "w") if i == 0 else open(options.per_segment_out, "a")
        if(i == 0): results.print_inferences_header(f_per_res, options.fasta)

        results.print_inferences(averaged_inferences[1], binary_inferences[1], 
            orig_headers, labels[1],
            clashes[1], f_per_res, per_res_sequences, options.fasta, "per-segment")

    if(options.per_res_out and options.plot_dir):
        results.plot_per_res_inferences(averaged_inferences[1], THRESHOLDS, 
        options.plot_dir, window_size=options.window_size_predictions)

    if(options.per_segment_out and options.plot_dir):
        results.plot_per_res_inferences(averaged_inferences[1], THRESHOLDS, 
            options.plot_dir, 
            smoothen=options.curve_smoothening, 
            window_size=options.window_size_predictions, 
            x_label=f"window (k={options.segment_size}) index", 
        title="Combined residue predictions")
